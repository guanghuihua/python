{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 It Starts with a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于浮点数是网络处理信息的方式，因此我们需要一种方法，将我们希望处理的真实世界的数据编码为网络可理解的数据，然后将输出解码为我们可以理解和使用的数据。\n",
    "深度神经网络通常在不同阶段学习将数据从一种形式转换为另一种形式，这意味着每个阶段转换的数据可以被认为是一个中间表征序列。\n",
    "一般来说，这些中间表征是浮点数的集合，它们描述输入的特征，并以一种有助于描述输入映射到神经网络输出的方式捕获数据的结构。\n",
    "开始将数据转换为浮点数输入之前，我们必须先对 PyTorch 如何处理和存储数据有深入的理解，诸如如何将数据处理作为输入、中间表征和输出。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在深度学习中，张量可以将向量和矩阵推广到任意维度.\n",
    "这个概念的另一个名称是多维数组，张量的维度与用来表示张量中标量值的索引数量一致。\n",
    "\n",
    "torch张量或Numpy数组通常是连续内存块的视图，每个字节都是32位bit的浮点数(4字节)\n",
    "这意味着存储1000,000个浮点数的一维张量恰好需要4000,000个连续字节\n",
    "在加上元数据的小开销，如维度和数字类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 1., 5., 3., 2., 1.])\n",
      "tensor([4., 1., 5., 3., 2., 1.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.0, 1.0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一个例子， 一个二维三角形，顶点坐标是(4,1), (5,3),(2,1)\n",
    "\n",
    "## 可以使用一维张量\n",
    "import torch\n",
    "points = torch.zeros(6)\n",
    "points[0] = 4.0\n",
    "points[1] = 1.0\n",
    "points[2] = 5.0\n",
    "points[3] = 3.0\n",
    "points[4] = 2.0\n",
    "points[5] = 1.0\n",
    "print(points)\n",
    "## 或者传一个python列表表达同样效果\n",
    "points = torch.tensor([4.0,1.0,5.0,3.0,2.0,1.0])\n",
    "print(points)\n",
    "## 为了得到第一个点的坐标，可以执行下面操作\n",
    "float(points[0]),float(points[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 1.],\n",
      "        [5., 3.],\n",
      "        [2., 1.]])\n",
      "tensor([4., 1.])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#也可以使用二维张量\n",
    "points = torch.tensor([[4.0,1.0],[5.0,3.0],[2.0,1.0]])\n",
    "print(points)\n",
    "print(points[0])\n",
    "print(points[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:**\n",
    "    注意到上面points[0]输出另外一个张量，新的张量是一个大小为2的一维张量，引用了张量points中第1行的值，\n",
    "    这是否意味着分配了一个新的内存块，将值复制到其中，并将新内存块包装在一个新的张量对象中返回？\n",
    "    答案是不！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2]) \n",
      "\n",
      "tensor([[5., 3.],\n",
      "        [2., 1.]])\n",
      "tensor([[5., 3.],\n",
      "        [2., 1.]])\n",
      "tensor([5., 2.])\n",
      "tensor([[[4., 1.],\n",
      "         [5., 3.],\n",
      "         [2., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "print(points.shape,\"\\n\")\n",
    "print(points[1:])\n",
    "print(points[1:,:]) # All rows after the first; all columns\n",
    "print(points[1:,0]) # All rows after the first; first column\n",
    "print(points[None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Named tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t = torch.randn(3,5,5) # shape [channels, rows, columns]\n",
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_t = torch.randn(2,3,5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive = img_t.mean(-3) # 倒着数，数到第三个\n",
    "batch_gray_naive = batch_t.mean(-3) # 倒着数，数到第三个\n",
    "img_gray_naive.shape, batch_gray_naive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze_(-1)\n",
    "unsqueezed_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color =  'red'> Remark </font >**\n",
    "\n",
    "unsqueeze()函数是PyTorch中的一个方法，它可以在张量的指定位置插入一个新的维度。\n",
    "\n",
    "例如，如果我们有一个形状为(3, 4)的张量，我们可以使用unsqueeze(0)在第0个位置插入一个新的维度，从而得到一个形状为(1, 3, 4)的张量。\n",
    "\n",
    "unsqueeze()函数的参数可以是正数或负数。如果参数是正数，则表示在指定的位置插入一个新的维度；\n",
    "\n",
    "如果参数是负数，则表示在指定的位置插入一个新的维度，但是该维度的索引是从张量的末尾开始计算的。\n",
    "\n",
    "例如，如果我们有一个形状为(3, 4)的张量，我们可以使用unsqueeze(-1)在最后一个位置插入一个新的维度，从而得到一个形状为(3, 4, 1)的张量。\n",
    "这种方式可以方便地将两个张量进行广播，从而使它们具有相同的形状。\n",
    "\n",
    "详细解释见<https://stackoverflow.com/questions/57237352/what-does-unsqueeze-do-in-pytorch/65831759#65831759>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[[True],\n",
      "         [True]],\n",
      "\n",
      "        [[True],\n",
      "         [True]]])\n",
      "tensor([[[True, True]],\n",
      "\n",
      "        [[True, True]]])\n",
      "tensor([[[True, True],\n",
      "         [True, True]]])\n",
      "tensor([[[ True,  True],\n",
      "         [False, False]],\n",
      "\n",
      "        [[False, False],\n",
      "         [ True,  True]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 2]),\n",
       " torch.Size([2, 1, 2]),\n",
       " torch.Size([2, 2, 1]),\n",
       " torch.Size([1, 2, 2]),\n",
       " torch.Size([2, 1, 2]),\n",
       " torch.Size([2, 2, 1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unsqueeze()的一个例子\n",
    "a = torch.tensor([[1,2],\n",
    "                 [3,4]])\n",
    "print(a.shape)\n",
    "print(torch.unsqueeze(a, 2) == torch.unsqueeze(a, -1))\n",
    "print(torch.unsqueeze(a, 1) == torch.unsqueeze(a, -2))\n",
    "print(torch.unsqueeze(a, 0) == torch.unsqueeze(a, -3))\n",
    "print(torch.unsqueeze(a, 1) == torch.unsqueeze(a, -3))\n",
    "a.unsqueeze(-3).shape,a.unsqueeze(-2).shape,a.unsqueeze(-1).shape,a.unsqueeze(0).shape,a.unsqueeze(1).shape,a.unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 将允许我们对相同形状的张量进行乘法运算，也允许与给定维度中其中一个操作数大小为 1 的张量进行运算。\n",
    "\n",
    "它还会自动附加大小为 1 的前导维度，这个特性被称为广播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 5, 5]), torch.Size([2, 3, 5, 5]), torch.Size([3, 1, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_weights = (img_t * unsqueezed_weights) # 单单从张量的shape角度看 [3,5,5] * [3,1,1] = [3,5,5]\n",
    "batch_weights = (batch_t * unsqueezed_weights)  # [2,3,5,5]*[3,1,1] = [2,3,5,5]\n",
    "img_gray_weighted = img_weights.sum(-3) #从末端开始对第三维（3 个通道）进行求和\n",
    "batch_gray_weighted = batch_weights.sum(-3)\n",
    "batch_weights.shape, batch_t.shape, unsqueezed_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 The tensor API\n",
    "\n",
    "The vast majority of operations on and between tensors are available in the torch module and can also be called as methods of a tensor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2]) torch.Size([2, 3])\n",
      "torch.Size([3, 2]) torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 例子，转置transpose\n",
    "## 注意区分 functions & methods\n",
    "a = torch.ones(3,2)\n",
    "a_t = torch.transpose(a, 0, 1)\n",
    "print(a.shape, a_t.shape)\n",
    "\n",
    "a = torch.ones(3,2)\n",
    "a_t = a.transpose(0,1)\n",
    "print(a.shape, a_t.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Tensors: Scenic views of storage\n",
    "\n",
    "张量中的值被分配到由 torch.Storage 实例所管理的连续内存块中。\n",
    "\n",
    "A storage is a one-dimensional array of numerical data: that is, a contiguous block of memory containing numbers of a given type, such as float (32 bits representing a floating-point number) or int64 (64 bits representing an integer).\n",
    "\n",
    "一个 PyTorch 的 Tensor 实例就是这样一个 Storage 实例的视图，该实例能够使用**偏移量**和**每个维度的步长**对该存储区进行**索引**\n",
    "\n",
    "多个张量可以索引同一存储区，即使它们索引到的数据不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.storage of tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Indexing into storage\n",
    "\n",
    "points = torch.tensor([[4.0,1.0],[5.0,3.0],[2.0,1.0]])\n",
    "points.storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points_storage[0] = 4.0\n",
      "points.storage()[1] = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-0bca5eb25bb4>:3: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  points_storage = points.storage()\n"
     ]
    }
   ],
   "source": [
    "##  index into a storage manually\n",
    "\n",
    "points_storage = points.storage()\n",
    "print(\"points_storage[0] = {}\".format(points_storage[0]))\n",
    "print(\"points.storage()[1] = {}\".format(points.storage()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 改变一个存储区的值导致与其关联的张量的内容发生变化\n",
    "\n",
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]]) \n",
    "points_storage = points.storage()\n",
    "points_storage[0] = 2.0\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor(6.)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "## zero_()方法将输入的所有元素归零，任何不带下画线的方法都不会改变源张量，而是返回一个新的张量\n",
    "\n",
    "a = torch.ones(3,2)\n",
    "a.zero_()\n",
    "print(a)\n",
    "\n",
    "b = torch.ones(3,2)\n",
    "print(b.sum())\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.8 Tensor metadata: Size, offset, and stride\n",
    "\n",
    "In order to index into a storage, tensors rely on a few pieces of information that, together with their storage, unequivocally(明确地；不含糊的) define them: **<font  color = 'red' >size , offset, and stride</font>**.\n",
    "\n",
    "**The size** (or shape, in NumPy parlance) is a tuple indicating how many elements across each dimension the tensor represents. \n",
    "\n",
    "**The storage offset** is the index in the storage corresponding to the first element in the tensor.\n",
    "\n",
    "**The stride** is the number of elements in the storage that need to be skipped over to obtain the next element along each dimension.\n",
    "\n",
    "<font color = 'red'>Accessing an element i, j in a 2D tensor results in accessing the storage_offset +\n",
    "stride[0] * i + stride[1] * j element in the storage. </font>\n",
    "\n",
    "This indirection between Tensor and Storage makes some operations inexpen-\n",
    "sive, like transposing a tensor or extracting a subtensor, because they do not lead to\n",
    "memory reallocations. Instead, they consist of allocating a new Tensor object with a\n",
    "different value for size, storage offset, or stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points.storage_offsets() = 0\n",
      "second_point.storage_offsets() = 2\n",
      "(1,)\n",
      "(2, 1)\n",
      "torch.Size([2])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]]) \n",
    "second_point = points[1]\n",
    "print(\"points.storage_offsets() = {}\\n\".format(points.storage_offset()),\n",
    "      \"second_point.storage_offsets() = {}\".format(second_point.storage_offset()), sep=\"\")\n",
    "\n",
    "print(second_point.stride())\n",
    "print(points.stride())\n",
    "\n",
    "print(second_point.size())\n",
    "print(second_point.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**This also means changing the subtensor will have a side effect on the original tensor**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]]) \n",
    "second_point = points[1].clone()\n",
    "second_point[0] = 10.0\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 1.],\n",
      "        [5., 3.],\n",
      "        [2., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5., 2.],\n",
       "        [1., 3., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "print(points)\n",
    "points_t = points.t()\n",
    "points_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.]]]),\n",
       " (20, 5, 1),\n",
       " torch.Size([3, 4, 5]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_t = torch.ones(3,4,5)\n",
    "some_t,some_t.stride(),some_t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4, 3]),\n",
       " (1, 5, 20),\n",
       " tensor([[[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transpose_t = some_t.transpose(0,2)\n",
    "transpose_t.shape,transpose_t.stride(),transpose_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8.4 Contiguous tensors\n",
    "\n",
    "In our case, points is contiguous, while its transpose is not.\n",
    "\n",
    "+ <font color=red> Attention: the tensor is contiguous rather than continuous!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.is_contiguous(),points_t.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 2),\n",
       "  4.0\n",
       "  1.0\n",
       "  5.0\n",
       "  3.0\n",
       "  2.0\n",
       "  1.0\n",
       " [torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6],\n",
       " tensor([[4., 5., 2.],\n",
       "         [1., 3., 1.]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The content of the tensor will be the same, but the stride will change, as will the storage.\n",
    "\n",
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points_t = points.t()\n",
    "points_t.stride(),points_t.storage(),points_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 1),\n",
       "  4.0\n",
       "  5.0\n",
       "  2.0\n",
       "  1.0\n",
       "  3.0\n",
       "  1.0\n",
       " [torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6],\n",
       " tensor([[4., 5., 2.],\n",
       "         [1., 3., 1.]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t_cont = points_t.contiguous()\n",
    "points_t_cont.stride(),points_t_cont.storage(),points_t_cont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Moving tensors to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# omission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
